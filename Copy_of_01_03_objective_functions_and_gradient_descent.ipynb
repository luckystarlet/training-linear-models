{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luckystarlet/training-linear-models/blob/main/Copy_of_01_03_objective_functions_and_gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEsTPX2vYPGO"
      },
      "source": [
        "# Objective Functions and Gradient Descent\n",
        "\n",
        "### 2025-09-03"
      ],
      "id": "tEsTPX2vYPGO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pJw3Jn5YPGQ"
      },
      "source": [
        "**Abstract**: In this session we introduce the notion of objective\n",
        "functions and show how they can be used in a simple optimisation systems\n",
        "based on gradients."
      ],
      "id": "3pJw3Jn5YPGQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Es8wXr6vYPGR"
      },
      "source": [
        "$$\n",
        "$$"
      ],
      "id": "Es8wXr6vYPGR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0g91trSYPGR"
      },
      "source": [
        "<!-- Do not edit this file locally. -->\n",
        "<!-- Do not edit this file locally. -->\n",
        "<!---->\n",
        "<!-- Do not edit this file locally. -->\n",
        "<!-- Do not edit this file locally. -->\n",
        "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
        "<!--\n",
        "\n",
        "-->"
      ],
      "id": "I0g91trSYPGR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umDaRp7nYPGR"
      },
      "source": [
        "## ML Foundations Course Notebook Setup\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_mlfc/includes/mlfc-notebook-setup.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_mlfc/includes/mlfc-notebook-setup.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "We install some bespoke codes for creating and saving plots as well as\n",
        "loading data sets."
      ],
      "id": "umDaRp7nYPGR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwVzigFkYPGS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install notutils\n",
        "%pip install pods\n",
        "%pip install mlai"
      ],
      "id": "HwVzigFkYPGS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLcrIZwbYPGT"
      },
      "outputs": [],
      "source": [
        "import notutils\n",
        "import pods\n",
        "import mlai\n",
        "import mlai.plot as plot"
      ],
      "id": "FLcrIZwbYPGT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHxmJZtCYPGT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams.update({'font.size': 22})"
      ],
      "id": "EHxmJZtCYPGT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVzzvUjSYPGT"
      },
      "source": [
        "<!--setupplotcode{import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "sns.set_context('paper')\n",
        "sns.set_palette('colorblind')}-->\n",
        "\n",
        "On Monday we introduce machine learning and motivate the importance of\n",
        "probability. We suggested that many machine learning algorithms can be\n",
        "motivated by considering a prediction funcation and an objective\n",
        "function. Together these form our mode that can be combined with data\n",
        "through computation and used to make predictions.\n",
        "\n",
        "We also motivated the importance of probability. We introduced Laplace’s\n",
        "Gremlin and suggested that probability is a way of representing our\n",
        "ignorance. But objective functions are not always motivated by\n",
        "probability. Today we consider the optimisation of objective functions.\n",
        "\n",
        "But before we start specifically on objective functions we consider the\n",
        "oldest machine learning algorithm, the perceptron."
      ],
      "id": "GVzzvUjSYPGT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV1vxZXNYPGU"
      },
      "source": [
        "## Introduction to Classification\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/classification-intro.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/classification-intro.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "Classification is perhaps the technique most closely assocated with\n",
        "machine learning. In the speech based agents, on-device classifiers are\n",
        "used to determine when the wake word is used. A wake word is a word that\n",
        "wakes up the device. For the Amazon Echo it is “Alexa,” for Siri it is\n",
        "“Hey Siri.” Once the wake word detected with a classifier, the speech\n",
        "can be uploaded to the cloud for full processing, the speech recognition\n",
        "stages.\n",
        "\n",
        "This isn’t just useful for intelligent agents, the UN global pulse\n",
        "project on public discussion on radio also uses [wake word detection for\n",
        "recording radio conversations](https://radio.unglobalpulse.net/uganda/).\n",
        "\n",
        "A major breakthrough in image classification came in 2012 with the\n",
        "ImageNet result of [Alex Krizhevsky, Ilya Sutskever and Geoff\n",
        "Hinton](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-)\n",
        "from the University of Toronto. ImageNet is a large data base of 14\n",
        "million images with many thousands of classes. The data is used in a\n",
        "community-wide challenge for object categorization. Krizhevsky et al\n",
        "used convolutional neural networks to outperform all previous approaches\n",
        "on the challenge. They formed a company which was purchased shortly\n",
        "after by Google. This challenge, known as object categorisation, was a\n",
        "major obstacle for practical computer vision systems. Modern object\n",
        "categorization systems are close to human performance.\n",
        "\n",
        "Machine learning problems normally involve a prediction function and an\n",
        "objective function. Regression is the case where the prediction function\n",
        "iss over the real numbers, so the codomain of the functions,\n",
        "$f(\\mathbf{X})$ was the real numbers or sometimes real vectors. The\n",
        "classification problem consists of predicting whether or not a\n",
        "particular example is a member of a particular class. So we may want to\n",
        "know if a particular image represents a digit 6 or if a particular user\n",
        "will click on a given advert. These are classification problems, and\n",
        "they require us to map to *yes* or *no* answers. That makes them\n",
        "naturally discrete mappings.\n",
        "\n",
        "In classification we are given an input vector, $\\mathbf{ x}$, and an\n",
        "associated label, $y$ which either takes the value $-1$ to represent\n",
        "*no* or $1$ to represent *yes*.\n",
        "\n",
        "In supervised learning the inputs, $\\mathbf{ x}$, are mapped to a label,\n",
        "$y$, through a function $f(\\cdot)$ that is dependent on a set of\n",
        "parameters, $\\mathbf{ w}$, $$\n",
        "y= f(\\mathbf{ x}; \\mathbf{ w}).\n",
        "$$ The function $f(\\cdot)$ is known as the *prediction function*. The\n",
        "key challenges are (1) choosing which features, $\\mathbf{ x}$, are\n",
        "relevant in the prediction, (2) defining the appropriate *class of\n",
        "function*, $f(\\cdot)$, to use and (3) selecting the right parameters,\n",
        "$\\mathbf{ w}$."
      ],
      "id": "gV1vxZXNYPGU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpVFI0D4YPGU"
      },
      "source": [
        "## Classification Examples\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/classification-examples.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/classification-examples.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "There are many difference examples of classification problems. One of\n",
        "theoldest is the classification of hand written digits from binary\n",
        "images. The MNIST data was for a long time considered one of the most\n",
        "difficult data sets.\n",
        "\n",
        "When we download this data we are making use of open source code,\n",
        "through the [scikit learn](https://scikit-learn.org/stable/) project,\n",
        "and the [OpenML](https://www.openml.org/) project which provides open\n",
        "access to data.\n",
        "\n",
        "This reflects a tradition of openness in machine learning that has\n",
        "enabled the tools to be deployed."
      ],
      "id": "RpVFI0D4YPGU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ixcOO_hYPGV"
      },
      "source": [
        "## Subsample of the MNIST Data\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_datasets/includes/mnist-digits-subsample-data.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/mnist-digits-subsample-data.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "We will look at a sub-sample of the MNIST digit data set.\n",
        "\n",
        "THE MNIST DATABASE of handwritten digits was created by Yann LeCun,\n",
        "Corinna Cortes and Christopher J. C. Burges. More details can be found\n",
        "at Yann’s site <http://yann.lecun.com/exdb/mnist/>\n",
        "\n",
        "The data is a modified version of a data set from the US National\n",
        "Institute of Standards and Technology ([NIST](https://www.nist.gov/).\n",
        "\n",
        "First load in the MNIST data set from scikit learn. This can take a\n",
        "little while because it’s large to download."
      ],
      "id": "1ixcOO_hYPGV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YsPkylCYPGV"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml"
      ],
      "id": "7YsPkylCYPGV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfYyFyEbYPGV"
      },
      "outputs": [],
      "source": [
        "mnist = fetch_openml('mnist_784')"
      ],
      "id": "lfYyFyEbYPGV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2bcmN5TYPGV"
      },
      "source": [
        "Sub-sample the dataset to make the training faster."
      ],
      "id": "w2bcmN5TYPGV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WwjZ138YPGV"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ],
      "id": "0WwjZ138YPGV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YZsmNkQYPGW"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "digits = [0,1,2,3,4]\n",
        "N_per_digit = 100\n",
        "Y = []\n",
        "labels = []\n",
        "for d in digits:\n",
        "    imgs = mnist['data'][mnist['target']==str(d)]\n",
        "    Y.append(imgs.loc[np.random.permutation(imgs.index)[:N_per_digit]])\n",
        "    labels.append(np.ones(N_per_digit)*d)\n",
        "Y = np.vstack(Y).astype(np.float64)\n",
        "labels = np.hstack(labels)\n",
        "Y /= 255"
      ],
      "id": "_YZsmNkQYPGW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVsnWCefYPGW"
      },
      "source": [
        "Now let’s visualise some examples from our subsampled dataset to get a\n",
        "sense of what the data looks like."
      ],
      "id": "cVsnWCefYPGW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6J4Fs4pYPGW"
      },
      "source": [
        "## MNIST Digit Examples\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_datasets/includes/mnist-digits-plot.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/mnist-digits-plot.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
      ],
      "id": "S6J4Fs4pYPGW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6dc9kJEYPGW"
      },
      "outputs": [],
      "source": [
        "import mlai\n",
        "import mlai.plot as plot\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "S6dc9kJEYPGW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jn8y5PRnYPGW"
      },
      "outputs": [],
      "source": [
        "# Visualize examples of each digit\n",
        "fig, axes = plt.subplots(5, 5, figsize=plot.big_figsize)\n",
        "\n",
        "for digit in digits:\n",
        "    # Get indices for this digit\n",
        "    digit_indices = np.where(labels == digit)[0]\n",
        "\n",
        "    # Show first 5 examples of this digit\n",
        "    for i in range(5):\n",
        "        row = int(digit)\n",
        "        col = i\n",
        "\n",
        "        # Reshape 784-dim vector back to 28x28 image\n",
        "        image = Y[digit_indices[i]].reshape(28, 28)\n",
        "\n",
        "        axes[row, col].imshow(image, cmap='gray')\n",
        "        axes[row, col].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "mlai.write_figure(\"mnist-digits-subsample-examples.svg\", directory=\"./datasets\")"
      ],
      "id": "jn8y5PRnYPGW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1PCWFe6YPGX"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//datasets/mnist-digits-subsample-examples.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>Examples of MNIST digits 0-4 from our subsampled dataset,\n",
        "showing 5 examples of each digit.</i>\n",
        "\n",
        "The visualization shows the variety in handwritten digits even within\n",
        "the same class. Each row represents a different digit (0 through 4), and\n",
        "each column shows a different example of that digit.\n",
        "\n",
        "While the MNIST data doesn’t capture all the nuances of the challenges\n",
        "we face in modern machine learning. It has been an important benchmark\n",
        "data set for and can still be a useful data set for learning about\n",
        "classfication algorithms."
      ],
      "id": "r1PCWFe6YPGX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn4YhcwIYPGX"
      },
      "source": [
        "## Hyperplane\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/classification.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/classification.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "The objective of classification is to predict the class the class label,\n",
        "$y_i$, given the features associated with that data point,\n",
        "$\\mathbf{ x}_i$, using the *prediction function*. If we are using a\n",
        "linear model, then we can define the prediction function as\\* Predict\n",
        "class label $y_i$ \\* Using data features $\\mathbf{ x}_i$ \\* Through the\n",
        "prediction function} $$\n",
        "f(x_i) = \\text{sign}\\left(\\mathbf{ w}^\\top \\mathbf{ x}_i + b\\right),\n",
        "$$ where the prediction here is `+1` for the positive class and `-1` for\n",
        "the negative class.\n",
        "\n",
        "In this linear model the decision boundary for classification is given\n",
        "by a *hyperplane*. The vector, $\\mathbf{ w}$, is the *[normal\n",
        "vector](http://en.wikipedia.org/wiki/Normal_(geometry))* to the\n",
        "hyperplane. Any hyperplane can be described by formula\n",
        "$\\mathbf{ w}^\\top \\mathbf{ x}= -b$.\n",
        "\n",
        "Note that this is the same linear form that underpins *linear\n",
        "regression* but here it is being used to define the hyperplane rather\n",
        "than the regression weights"
      ],
      "id": "bn4YhcwIYPGX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yut6iG0OYPGX"
      },
      "source": [
        "## Toy Data\n",
        "\n",
        "We’ll consider a toy data set and a decision boundary that separates red\n",
        "crosses from green circles."
      ],
      "id": "Yut6iG0OYPGX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMxvsgaQYPGX"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ],
      "id": "rMxvsgaQYPGX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gUDvpSBYPGX"
      },
      "outputs": [],
      "source": [
        "np.random.seed(seed=1000001)\n",
        "x_plus = np.random.normal(loc=1.3, size=(30, 2))\n",
        "x_minus = np.random.normal(loc=-1.3, size=(30, 2))"
      ],
      "id": "-gUDvpSBYPGX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhCVuhxbYPGX"
      },
      "outputs": [],
      "source": [
        "import mlai\n",
        "import mlai.plot as plot\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "MhCVuhxbYPGX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-_me0NbYPGY"
      },
      "outputs": [],
      "source": [
        "# plot data\n",
        "fig, ax = plt.subplots(figsize=plot.big_figsize)\n",
        "ax.plot(x_plus[:, 0], x_plus[:, 1], 'rx')\n",
        "ax.plot(x_minus[:, 0], x_minus[:, 1], 'go')\n",
        "\n",
        "plt.tight_layout()\n",
        "mlai.write_figure(\"artificial-classification-example.svg\", directory=\"./ml\")"
      ],
      "id": "7-_me0NbYPGY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRNJp6SeYPGY"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/artificial-classification-example.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>Red crosses and green circles are sampled from two separate\n",
        "Gaussian distributions with 30 examples of each.</i>"
      ],
      "id": "fRNJp6SeYPGY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGoqdSUzYPGY"
      },
      "source": [
        "## The Perceptron\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/perceptron.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/perceptron.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "The Perceptron (Rosenblatt (1958)) is perhaps the oldest machine\n",
        "learning algorithm. The algorithm was inspired by ideas of statistical\n",
        "pattern recognition and was developed by Frank Rosenblatt at Cornell\n",
        "University in the 1950s."
      ],
      "id": "VGoqdSUzYPGY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KENKJbYKYPGY"
      },
      "source": [
        "## Mathematical Drawing of Decision Boundary\n",
        "\n",
        "We draw a hyper plane at decision boundary. The *decision boundary* is\n",
        "where a point moves from being classified as -1 to +1. For our two\n",
        "dimensional feature space it is defined by $$\n",
        "\\text{sign}(\\mathbf{ x}^\\top \\mathbf{ w}) = \\text{sign}(w_0 + w_1x_{i,1} + w_2 x_{i, 2})\n",
        "$$ where $x_{i, 1}$ is first feature $x_{i, 2}$ is second feature and\n",
        "assume $x_{0,i}=1$, in other words it plays the role of the bias, $b$.\n",
        "So setting $w_0 = b$ we have $$\n",
        "\\text{sign}\\left(w_1 x_{i, 1} + w_2 x_{i, 2} + b\\right)\n",
        "$$"
      ],
      "id": "KENKJbYKYPGY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6Z8hjT8YPGY"
      },
      "source": [
        "## Reminder: Equation of Plane\n",
        "\n",
        "The prediction function is $$\n",
        "\\text{sign}\\left(w_1 x_{i, 1} + w_2 x_{i, 2} + b\\right)\n",
        "$$ and the equation of a plane is $$\n",
        "w_1 x_{i, 1} + w_2 x_{i, 2} + b = 0\n",
        "$$ or $$\n",
        "w_1 x_{i, 1} + w_2 x_{i, 2} = -b.\n",
        "$$ This is also the precise point that the argument of the\n",
        "$\\text{sign}(\\cdot)$ function is zero in our Perceptron algorithm. So\n",
        "it’s the point at which a point switches from being classified negative\n",
        "to being classified positive (or vice versa).\n",
        "\n",
        "The next step is to initialise the Preceptron model and draw a decision\n",
        "boundary."
      ],
      "id": "O6Z8hjT8YPGY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ42dm_8YPGY"
      },
      "source": [
        "## Perceptron Algorithm: Initialisation Maths\n",
        "\n",
        "There’s no single way to initialise an algorithm like the Perceptron,\n",
        "but here’s a way that is informative because it helps see why the\n",
        "Perceptron works. We will take a randomly chosen data point, $i$, and\n",
        "set $$\n",
        "\\mathbf{ w}= y_i \\mathbf{ x}_i.\n",
        "$$ Why is this sensible? Well if the predicted label of the $i$th point\n",
        "is $$\n",
        "\\text{sign}(\\mathbf{ w}^\\top\\mathbf{ x}_i)\n",
        "$$ then setting $\\mathbf{ w}$ to $y_i\\mathbf{ x}_i$ implies $$\n",
        "\\text{sign}(\\mathbf{ w}^\\top\\mathbf{ x}_i) = \\text{sign}(y_i\\mathbf{ x}_i^\\top \\mathbf{ x}_i) = y_i\n",
        "$$ which means that the point we’ve selected will be correctly\n",
        "classified.\n",
        "\n",
        "For simple data sets like our artifical red and green crosses, this\n",
        "algorithm can actually be so good that it gets the decision boundary\n",
        "very close from the start."
      ],
      "id": "eJ42dm_8YPGY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nc1bRKpuYPGZ"
      },
      "outputs": [],
      "source": [
        "import mlai"
      ],
      "id": "Nc1bRKpuYPGZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGpXqJ4pYPGZ"
      },
      "outputs": [],
      "source": [
        "%load -n mlai.init_perceptron"
      ],
      "id": "fGpXqJ4pYPGZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX1tbnf5YPGZ"
      },
      "source": [
        "## Drawing Decision Boundary"
      ],
      "id": "KX1tbnf5YPGZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLJPAF4aYPGZ"
      },
      "outputs": [],
      "source": [
        "import mlai.plot as plot"
      ],
      "id": "iLJPAF4aYPGZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6V5eJ5ArYPGZ"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(1, 2, figsize=(14,7))\n",
        "w, b, x_select = init_perceptron(x_plus, x_minus)\n",
        "handle = plot.init_perceptron(f, ax, x_plus, x_minus, w, b)\n",
        "mlai.write_figure(\"perceptron_init.svg\", directory=\"./ml\")"
      ],
      "id": "6V5eJ5ArYPGZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hnyNDowYPGZ"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/perceptron_init.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>Initial perceptron setup showing data points and initial\n",
        "decision boundary.</i>\n",
        "\n",
        "The decision boundary is where the output of the function changes from\n",
        "-1 to +1 (or vice versa) so it’s the point at which the argument of the\n",
        "$\\text{sign}$ function is zero. So in other words, the decision boundary\n",
        "is given by the *line* defined by $x_1 w_1 + x_2 w_2 = -b$ (where we\n",
        "have dropped the index $i$ for convenience). In this two dimensional\n",
        "space the decision boundary is defined by a line. In a three dimensional\n",
        "space it would be defined by a *plane* and in higher dimensional spaces\n",
        "it is defined by something called a\n",
        "[*hyperplane*](http://en.wikipedia.org/wiki/Hyperplane). This equation\n",
        "is therefore often known as the *separating hyperplane* because it\n",
        "defines the hyperplane that separates the data. To draw it in 2-D we can\n",
        "choose some values to plot from $x_1$ and then find the corresponding\n",
        "values for $x_2$ to plot using the rearrangement of the hyperplane\n",
        "formula as follows $$\n",
        "x_2 = -\\frac{(b+x_1w_1)}{w_2}\n",
        "$$ Of course, we can also choose to specify the values for $x_2$ and\n",
        "compute the values for $x_1$ given the values for $x_2$, $$\n",
        "x_1 = -\\frac{b + x_2w_2}{w_1}\n",
        "$$"
      ],
      "id": "3hnyNDowYPGZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgGuU_l7YPGe"
      },
      "source": [
        "## Switching Formulae\n",
        "\n",
        "Sometimes we need to use the first formula, and sometimes we need to use\n",
        "the second. Which formula we use depends on how the separating\n",
        "hyperplane leaves the plot.\n",
        "\n",
        "We want to draw the separating hyperplane in the bounds of the plot\n",
        "which is showing our data. To think about which equation to use, let’s\n",
        "consider two separate situations (actually there are a few more).\n",
        "\n",
        "1.  If the separating hyperplane leaves the top and bottom of the plot\n",
        "    then we want to plot a line with values in the $y$ direction (given\n",
        "    by $x_2$) given by the upper and lower limits of our plot. The\n",
        "    values in the $x$ direction can then be computed from the formula\n",
        "    for the plane.\n",
        "\n",
        "2.  Conversely if the line leaves the sides of the plot then we want to\n",
        "    plot a line with values in the $x$ direction given by the limits of\n",
        "    the plot. Then the values in the $y$ direction can be computed from\n",
        "    the formula. Whether the line leaves the top/bottom or the sides of\n",
        "    the plot is dependent on the relative values of $w_1$ and $w_2$.\n",
        "\n",
        "This motivates a simple `if` statement to check which situation we’re\n",
        "in."
      ],
      "id": "NgGuU_l7YPGe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zssDNbj0YPGf"
      },
      "outputs": [],
      "source": [
        "import mlai"
      ],
      "id": "zssDNbj0YPGf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SPT58ZJYPGf"
      },
      "outputs": [],
      "source": [
        "%load -n mlai.update_perceptron"
      ],
      "id": "6SPT58ZJYPGf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUABBu8iYPGf"
      },
      "source": [
        "The code for plotting the perceptron boundary is also provided. Here\n",
        "note how many more lines are required for plotting than are required for\n",
        "updating! The plotting code runs an entire optimisation of the\n",
        "Perceptron algorithm showing the histrogram of points projected onto the\n",
        "*normal vector*, $\\mathbf{ w}$."
      ],
      "id": "LUABBu8iYPGf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3YESsXyYPGf"
      },
      "outputs": [],
      "source": [
        "import mlai.plot as plot"
      ],
      "id": "L3YESsXyYPGf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rscM9060YPGf"
      },
      "outputs": [],
      "source": [
        "plots = plot.perceptron(x_plus, x_minus, seed=1000001, diagrams='./mlai')"
      ],
      "id": "rscM9060YPGf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkHH2i2RYPGg"
      },
      "outputs": [],
      "source": [
        "import notutils as nu"
      ],
      "id": "HkHH2i2RYPGg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulCK21fsYPGg"
      },
      "outputs": [],
      "source": [
        "nu.display_plots('perceptron{samp:0>3}.svg', directory='./ml', samp=(0, plots))"
      ],
      "id": "ulCK21fsYPGg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAWyS3WCYPGg"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/perceptron014.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>The perceptron decision boundary.</i>"
      ],
      "id": "WAWyS3WCYPGg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjLozUwZYPGg"
      },
      "source": [
        "## Perceptron Reflection\n",
        "\n",
        "It’s worth having some reflections on the Perceptron and what it does.\n",
        "The algorithm is simple enough that you can go through the exact\n",
        "updates. What is it doing? When will it fail? What happens when you\n",
        "can’t separate the classes with a linear hyperplane? Why does this stop\n",
        "the algorithm converging? How might you fix the algorithm to make it\n",
        "converge?"
      ],
      "id": "SjLozUwZYPGg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg1Sr4-XYPGg"
      },
      "source": [
        "## The Objective Function\n",
        "\n",
        "Another question is where is the objective function? I like the\n",
        "Percptron because if you study the algorithm you get an insight into\n",
        "what its doing and that gives an intuition about how its updating the\n",
        "parameters. But it’s not easy to see how that connects to an objective\n",
        "function. Very often to define an algorithm its much easier to start\n",
        "with an *objective function* (otherwise known as a loss function, an\n",
        "error function or a cost function)."
      ],
      "id": "tg1Sr4-XYPGg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpdkstHSYPGg"
      },
      "source": [
        "## Objective Functions and Regression\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/regression.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/regression.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "In classification we’re predicting discrete values, we use the sign\n",
        "function in the Perceptron algorithm to convert from a continuous value\n",
        "to a discrete by interpreting the continuos values as defining a\n",
        "(hyper)plane. In regression we’re trying to predict a continuous\n",
        "real-valued output rather than a discrete class label. Our goal is to\n",
        "find a function that maps input features to real-valued predictions.\n",
        "\n",
        "For linear regression, we assume a simple linear relationship between\n",
        "input and output. For a single feature, our prediction function takes\n",
        "the form $$\n",
        "f(x_i) = mx_i + c\n",
        "$$ where $m$ is the slope and $c$ is the intercept. How should we find\n",
        "the best values for these parameters?\n",
        "\n",
        "This is where the objective function comes in. We need a way to measure\n",
        "how well our current parameter values fit the data. The least squares\n",
        "objective function provides a principled approach by measuring the sum\n",
        "of squared differences between our predictions and the true values $$\n",
        "E(m, c) = \\sum_{i=1}^n(y_i - f(x_i))^2\n",
        "$$ The algorithm’s job is to find the values of $m$ and $c$ that\n",
        "minimize this error function.\n",
        "\n",
        "To demonstrate how regression works, we’ll create an artificial dataset\n",
        "where we know the true underlying relationship. This allows us to test\n",
        "whether our algorithm can recover the parameters we used to generate the\n",
        "data.\n",
        "\n",
        "We start by creating some random input values from a normal\n",
        "distribution. These will serve as our feature values $x_i$."
      ],
      "id": "QpdkstHSYPGg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imbXUzuUYPGh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import mlai"
      ],
      "id": "imbXUzuUYPGh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAjgBTxsYPGh"
      },
      "outputs": [],
      "source": [
        "x = np.random.normal(size=(4, 1))"
      ],
      "id": "xAjgBTxsYPGh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xiCdnFLYPGh"
      },
      "source": [
        "We now need to decide on a *true* value for $m$ and a *true* value for\n",
        "$c$ to use for generating the data. In real-world scenarios, we don’t\n",
        "know these values - that’s what we’re trying to learn. But for this\n",
        "demonstration, we’ll choose specific values so we can verify our\n",
        "algorithm works correctly."
      ],
      "id": "2xiCdnFLYPGh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRUfZAp1YPGi"
      },
      "outputs": [],
      "source": [
        "m_true = 1.4\n",
        "c_true = -3.1"
      ],
      "id": "nRUfZAp1YPGi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0qPcZmGYPGi"
      },
      "source": [
        "Now we can generate our target values using the linear relationship. The\n",
        "mathematical formula $y_i = mx_i + c$ translates directly into code.\n",
        "This creates an exact linear relationship - each point will lie exactly\n",
        "on the line defined by our chosen slope and intercept."
      ],
      "id": "U0qPcZmGYPGi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EgT5n5ZYPGi"
      },
      "outputs": [],
      "source": [
        "y = m_true*x+c_true"
      ],
      "id": "7EgT5n5ZYPGi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx1Ul4AcYPGi"
      },
      "source": [
        "Plotting our artificial data helps us visualise the linear relationship\n",
        "we’ve created. Since we generated the data deterministically using our\n",
        "linear function, all points should lie exactly on a straight line. This\n",
        "gives us a baseline to understand how the algorithm should perform in\n",
        "the ideal case."
      ],
      "id": "yx1Ul4AcYPGi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9J8brYcYPGi"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "id": "_9J8brYcYPGi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfV4RXJ6YPGj"
      },
      "outputs": [],
      "source": [
        "plt.plot(x, y, 'r.', markersize=10) # plot data as red dots\n",
        "plt.xlim([-3, 3])\n",
        "mlai.write_figure(filename='regression.svg', directory='./ml', transparent=True)"
      ],
      "id": "vfV4RXJ6YPGj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5BELoZwYPGj"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/regression.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>A simple linear regression.</i>\n",
        "\n",
        "As expected, these points lie exactly on a straight line since we\n",
        "generated them deterministically. However, real-world data is rarely\n",
        "this perfect. There are always measurement errors, unknown factors, and\n",
        "random variations that cause data to deviate from perfect relationships."
      ],
      "id": "m5BELoZwYPGj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8MqZhvHYPGj"
      },
      "source": [
        "## Noise Corrupted Plot\n",
        "\n",
        "To make our artificial dataset more realistic, we add Gaussian noise to\n",
        "our target values. This simulates the random variations we’d encounter\n",
        "in real data. The noise has zero mean, so it doesn’t systematically bias\n",
        "our data, but it does add random scatter around the true linear\n",
        "relationship.\n",
        "\n",
        "The standard deviation of the noise (0.5 in this case) controls how much\n",
        "the data deviates from the perfect line. Larger noise values make the\n",
        "regression problem more challenging, while smaller values keep the data\n",
        "closer to the underlying linear trend."
      ],
      "id": "T8MqZhvHYPGj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rc8JJR3PYPGj"
      },
      "outputs": [],
      "source": [
        "noise = np.random.normal(scale=0.5, size=(4, 1)) # standard deviation of the noise is 0.5\n",
        "y = m_true*x + c_true + noise\n",
        "plt.plot(x, y, 'r.', markersize=10)\n",
        "plt.xlim([-3, 3])\n",
        "mlai.write_figure(filename='regression_noise.svg', directory='./ml', transparent=True)"
      ],
      "id": "Rc8JJR3PYPGj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJMbASFhYPGj"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/regression_noise.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>A simple linear regression with noise.</i>"
      ],
      "id": "tJMbASFhYPGj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2TY2T-4YPGk"
      },
      "source": [
        "## Contour Plot of Error Function\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/regression-contour-plot.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/regression-contour-plot.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "To understand how the least squares algorithm works, it’s helpful to\n",
        "visualize the error function as a surface in parameter space. Since we\n",
        "have two parameters ($m$ and $c$), our error function $E(m, c)$ defines\n",
        "a surface in 3D space where the height at any point represents the error\n",
        "for that combination of parameters.\n",
        "\n",
        "The global minimum of this surface is given by the optimal parameter\n",
        "values that best fit our data according to the least squares objective.\n",
        "By visualising this surface through contour plots, we can gain intuition\n",
        "about the optimization landscape and understand why gradient-based\n",
        "methods work effectively for this problem.\n",
        "\n",
        "First, we create vectors of parameter values to explore around the true\n",
        "values we used to generate the data. We sample points in a range around\n",
        "the true parameters to see how the error function behaves in the local\n",
        "neighborhood."
      ],
      "id": "W2TY2T-4YPGk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNXv2t85YPGk"
      },
      "outputs": [],
      "source": [
        "# create an array of linearly separated values around m_true\n",
        "m_vals = np.linspace(m_true-3, m_true+3, 100)\n",
        "# create an array of linearly separated values around c_true\n",
        "c_vals = np.linspace(c_true-3, c_true+3, 100)"
      ],
      "id": "fNXv2t85YPGk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBQgfHuAYPGk"
      },
      "source": [
        "Next, we create a 2D grid from these parameter vectors. This grid allows\n",
        "us to evaluate the error function at every combination of $m$ and $c$\n",
        "values, giving us a complete picture of the error surface over the\n",
        "parameter space we’re exploring."
      ],
      "id": "BBQgfHuAYPGk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iKsjr42YPGk"
      },
      "outputs": [],
      "source": [
        "m_grid, c_grid = np.meshgrid(m_vals, c_vals)"
      ],
      "id": "8iKsjr42YPGk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP1RJlHdYPGk"
      },
      "source": [
        "Now we compute the error function at each combination of parameters. For\n",
        "each point in our grid, we: 1. Use the parameter values to make\n",
        "predictions: $\\hat{y}_i = m \\cdot x_i + c$  \n",
        "2. Calculate the squared errors: $(y_i - \\hat{y}_i)^2$ 3. Sum these\n",
        "squared errors to get the total error for that parameter combination\n",
        "\n",
        "This gives us the complete error surface that we can then visualize. The\n",
        "nested loop structure evaluates the sum of squared errors formula at\n",
        "each $(m, c)$ coordinate in our grid."
      ],
      "id": "MP1RJlHdYPGk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRrlwWUJYPGl"
      },
      "outputs": [],
      "source": [
        "E_grid = np.zeros((100, 100))\n",
        "for i in range(100):\n",
        "    for j in range(100):\n",
        "        E_grid[i, j] = ((y - m_grid[i, j]*x - c_grid[i, j])**2).sum()"
      ],
      "id": "qRrlwWUJYPGl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUvfy70AYPGl"
      },
      "source": [
        "With our error surface computed, we can now create a contour plot to\n",
        "visualize the optimization landscape. A contour plot shows lines of\n",
        "equal error value, similar to elevation contours on a topographic map.\n",
        "\n",
        "Insights from this visualisation include: - *Bowl-shaped surface*: For\n",
        "linear regression with least squares, the error surface is a smooth,\n",
        "convex bowl with a unique global minimum - *Contour lines*: Each contour\n",
        "represents parameter combinations that yield the same error value -\n",
        "*Minimum location*: The centre of the concentric ellipses shows where\n",
        "the error is minimized - this should be close to our true parameter\n",
        "values\n",
        "\n",
        "This visualisation helps explain why least squares regression has nice\n",
        "mathematical properties and why optimisation algorithms converge\n",
        "reliably to the solution."
      ],
      "id": "iUvfy70AYPGl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zmBMEXbYPGl"
      },
      "outputs": [],
      "source": [
        "import mlai.plot as plot\n",
        "import mlai"
      ],
      "id": "3zmBMEXbYPGl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsW3L_vOYPGl"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=(5,5))\n",
        "plot.regression_contour(f, ax, m_vals, c_vals, E_grid)\n",
        "mlai.write_figure(filename='regression_contour.svg', directory='./ml')"
      ],
      "id": "MsW3L_vOYPGl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUe39TU-YPGl"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/regression_contour.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>Contours of the objective function for linear regression by\n",
        "minimizing least squares.</i>\n",
        "\n",
        "The contour plot reveals the characteristic elliptical shape of the\n",
        "least squares error surface. The concentric ellipses represent\n",
        "increasing levels of error as we move away from the optimal parameters.\n",
        "\n",
        "Key observations from this visualization: - *Convex optimisation*: The\n",
        "smooth, bowl-shaped surface guarantees that any local minimum is also\n",
        "the global minimum - *Parameter sensitivity*: The shape of the ellipses\n",
        "tells us how sensitive the error is to changes in each parameter -\n",
        "*Optimization efficiency*: The regular, predictable shape means we can\n",
        "develop optimisation methods that will converge quickly and reliably -\n",
        "*True parameter location*: The minimum should occur very close to our\n",
        "known true values ($m_{true} = 1.4$, $c_{true} = -3.1$)\n",
        "\n",
        "*Warning:* This visualisation is great for giving some intuition, but\n",
        "can be quite misleading about how these objective funcitons look in very\n",
        "high dimensions. Unfortunately high dimensions are much harder to\n",
        "visualise."
      ],
      "id": "dUe39TU-YPGl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmXXd_WRYPGm"
      },
      "source": [
        "## Steepest Descent\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/regression-gradient-descent.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/regression-gradient-descent.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "Now that we understand the shape of the error surface from our contour\n",
        "plot, we need an algorithm to find the minimum. Gradient descent is one\n",
        "of the most fundamental optimization algorithms in machine learning.\n",
        "\n",
        "The intuition behind gradient descent is simple: imagine you’re standing\n",
        "on a hillside in fog and want to reach the bottom. Even though you can’t\n",
        "see the whole landscape, you can feel the steepest downward slope at\n",
        "your current position. By repeatedly taking steps in the steepest\n",
        "downward direction, you’ll eventually reach the bottom.\n",
        "\n",
        "Mathematically, the gradient points in the direction of steepest ascent,\n",
        "so we move in the negative gradient direction to descend toward the\n",
        "minimum. The algorithm works by: 1. Starting with an initial guess for\n",
        "our parameters 2. Computing the gradient (slope) of the error function\n",
        "at that point 3. Taking a small step in the opposite direction of the\n",
        "gradient 4. Repeating until we converge to the minimum"
      ],
      "id": "HmXXd_WRYPGm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLqwM96_YPGm"
      },
      "source": [
        "## Algorithm\n",
        "\n",
        "The first step in gradient descent is to initialise our parameters with\n",
        "some starting values. The choice of initial values can affect how\n",
        "quickly the algorithm converges, although because of the simple shape of\n",
        "the linear regression objective (known as a convex optimisation), any\n",
        "starting point should eventually reach the global minimum.\n",
        "\n",
        "Here we’re starting with $m = 0$ (no slope) and $c = -5$ (a negative\n",
        "intercept). These values are deliberately chosen to be different from\n",
        "our true values ($m_{true} = 1.4$, $c_{true} = -3.1$) so we can see the\n",
        "algorithm as it navigates toward the correct solution. Normally you\n",
        "would initialise with smaller values closer to the origin."
      ],
      "id": "rLqwM96_YPGm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87VyGZjuYPGm"
      },
      "outputs": [],
      "source": [
        "m_star = 0.0\n",
        "c_star = -5.0"
      ],
      "id": "87VyGZjuYPGm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNEJWvbrYPGm"
      },
      "source": [
        "## Offset Gradient\n",
        "\n",
        "To implement gradient descent, we need to compute the partial\n",
        "derivatives of our error function with respect to each parameter. We’ll\n",
        "start with the gradient with respect to the intercept parameter $c$.\n",
        "\n",
        "Our error function is $$E(m, c) = \\sum_{i=1}^n(y_i - mx_i - c)^2$$\n",
        "\n",
        "To find how the error changes with respect to $c$, we use the chain\n",
        "rule. Each squared term contributes to the gradient, and since $c$\n",
        "appears in every term with a coefficient of $-1$, the partial derivative\n",
        "becomes:\n",
        "$$\\frac{\\text{d}E(m, c)}{\\text{d} c} = -2\\sum_{i=1}^n(y_i - mx_i - c)$$\n",
        "\n",
        "The negative sign comes from differentiating the $-c$ term inside the\n",
        "squared expression, and the factor of 2 comes from differentiating the\n",
        "square."
      ],
      "id": "GNEJWvbrYPGm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_06HrmNYPGm"
      },
      "outputs": [],
      "source": [
        "c_grad = -2*(y-m_star*x - c_star).sum()\n",
        "print(\"Gradient with respect to c is \", c_grad)"
      ],
      "id": "3_06HrmNYPGm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlAqUlaDYPGn"
      },
      "source": [
        "## Deriving the Gradient\n",
        "\n",
        "To understand how we derived the gradient formula, let’s work through\n",
        "the calculus step by step.\n",
        "\n",
        "The error function has the form:\n",
        "$$E(m, c) = \\sum_{i=1}^n(y_i - mx_i - c)^2$$\n",
        "\n",
        "For the derivative with respect to $c$, note that $c$ appears in every\n",
        "term of the sum. For each term $(y_i - mx_i - c)^2$, we apply the chain\n",
        "rule:\n",
        "\n",
        "1.  *Outer function*: The derivative of $u^2$ is $2u$\n",
        "2.  *Inner function*: The derivative of $(y_i - mx_i - c)$ with respect\n",
        "    to $c$ is $-1$\n",
        "\n",
        "Combining these:\n",
        "$\\frac{d}{dc}[(y_i - mx_i - c)^2] = 2(y_i - mx_i - c) \\cdot (-1)$\n",
        "\n",
        "Since this applies to all terms in the sum, we get:\n",
        "$$\\frac{\\text{d}E(m, c)}{\\text{d} c} = -2\\sum_{i=1}^n(y_i - mx_i - c)$$"
      ],
      "id": "tlAqUlaDYPGn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeVzL1iGYPGn"
      },
      "source": [
        "## Slope Gradient\n",
        "\n",
        "The gradient with respect to the slope parameter $m$ follows the same\n",
        "chain rule approach, but now $m$ appears multiplied by $x_i$ in each\n",
        "term.\n",
        "\n",
        "For the derivative with respect to $m$, each term $(y_i - mx_i - c)^2$\n",
        "contributes: 1. *Outer function*: Still $2u$ where\n",
        "$u = (y_i - mx_i - c)$ 2. *Inner function*: The derivative of\n",
        "$(y_i - mx_i - c)$ with respect to $m$ is $-x_i$\n",
        "\n",
        "This gives us:\n",
        "$$\\frac{\\text{d}E(m, c)}{\\text{d} m} = -2\\sum_{i=1}^nx_i(y_i - mx_i - c)$$\n",
        "\n",
        "Notice that each term is weighted by $x_i$, which makes intuitive sense:\n",
        "data points with larger input values have more influence on the slope\n",
        "parameter."
      ],
      "id": "VeVzL1iGYPGn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgmSkrQuYPGn"
      },
      "outputs": [],
      "source": [
        "m_grad = -2*(x*(y-m_star*x - c_star)).sum()\n",
        "print(\"Gradient with respect to m is \", m_grad)"
      ],
      "id": "bgmSkrQuYPGn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiJ2KL5BYPGn"
      },
      "source": [
        "## Update Equations\n",
        "\n",
        "Now that we have computed both gradients, we can update our parameter\n",
        "estimates. The key insight is that we don’t want to simply subtract the\n",
        "full gradient from our current parameter values - this would likely\n",
        "cause us to overshoot the minimum.\n",
        "\n",
        "Instead, we take small steps in the negative gradient direction. This is\n",
        "crucial because: 1. *Gradient changes*: As we move through parameter\n",
        "space, the gradient itself changes, so we need to recompute it\n",
        "frequently 2. *Overshooting*: Large steps might cause us to jump over\n",
        "the minimum and potentially diverge 3. *Local information*: The gradient\n",
        "only gives us local information about the slope, not global information\n",
        "about the entire surface\n",
        "\n",
        "The size of the step we take is controlled by the learning rate, which\n",
        "we’ll introduce shortly."
      ],
      "id": "uiJ2KL5BYPGn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tGufaLpYPGn"
      },
      "source": [
        "## Move in Direction of Gradient\n",
        "\n",
        "Let’s visualize what a single gradient descent step looks like on our\n",
        "error surface. The plot shows our current parameter position as a green\n",
        "star and the direction we should move (negative gradient direction) as\n",
        "an arrow.\n",
        "\n",
        "The arrow points toward lower error values, following the steepest\n",
        "descent path from our current location. Notice how the arrow is\n",
        "perpendicular to the contour lines - this is always true for gradients,\n",
        "which by definition point in the direction of steepest increase (and\n",
        "thus their negative points in the direction of steepest decrease).\n",
        "\n",
        "The length and direction of this arrow tell us: - *Direction*: Where to\n",
        "move in parameter space - *Magnitude*: How steep the slope is (longer\n",
        "arrows mean steeper slopes) - *Step size*: We scale the arrow by the\n",
        "learning rate to determine our actual step size"
      ],
      "id": "6tGufaLpYPGn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXAFKr1PYPGo"
      },
      "outputs": [],
      "source": [
        "import mlai.plot as plot"
      ],
      "id": "sXAFKr1PYPGo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRYWUPhQYPGo"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=plot.big_figsize)\n",
        "plot.regression_contour(f, ax, m_vals, c_vals, E_grid)\n",
        "ax.plot(m_star, c_star, 'g*', markersize=20)\n",
        "ax.arrow(m_star, c_star, -m_grad*0.1, -c_grad*0.1, head_width=0.2)\n",
        "mlai.write_figure(filename='regression_contour_step001.svg', directory='./ml/', transparent=True)"
      ],
      "id": "iRYWUPhQYPGo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtpqXGW0YPGo"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/regression_contour_step001.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>Single update descending the contours of the error surface\n",
        "for regression.</i>"
      ],
      "id": "XtpqXGW0YPGo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy6imp3RYPGo"
      },
      "source": [
        "## Update Equations\n",
        "\n",
        "The crucial hyperparameter in gradient descent is the learning rate\n",
        "$\\eta$, which controls how big steps we take in the direction of the\n",
        "negative gradient. The learning rate is a balancing act:\n",
        "\n",
        "-   *Too large*: We might overshoot the minimum, potentially causing the\n",
        "    algorithm to diverge or oscillate wildly\n",
        "-   *Too small*: The algorithm will converge very slowly, requiring many\n",
        "    iterations to reach the minimum\n",
        "-   *Just right*: We make steady progress toward the minimum without\n",
        "    overshooting\n",
        "\n",
        "The update equations formally incorporate the learning rate:"
      ],
      "id": "Yy6imp3RYPGo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOT-3CZ4YPGo"
      },
      "source": [
        "## Update Code\n",
        "\n",
        "Let’s implement the parameter updates in code. We choose a learning rate\n",
        "of 0.01, which is small enough to ensure stable convergence but large\n",
        "enough to make reasonable progress.\n",
        "\n",
        "The code shows the before and after parameter values, demonstrating how\n",
        "a single gradient descent step moves us closer to the optimal solution.\n",
        "After this update, we would recompute the gradients at our new position\n",
        "and repeat the process."
      ],
      "id": "kOT-3CZ4YPGo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FvWp4hpYPGo"
      },
      "outputs": [],
      "source": [
        "print(\"Original m was\", m_star, \"and original c was\", c_star)\n",
        "learn_rate = 0.01\n",
        "c_star = c_star - learn_rate*c_grad\n",
        "m_star = m_star - learn_rate*m_grad\n",
        "print(\"New m is\", m_star, \"and new c is\", c_star)"
      ],
      "id": "0FvWp4hpYPGo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApklCmPpYPGp"
      },
      "source": [
        "## Iterating Updates\n",
        "\n",
        "One gradient step only takes us partway to the minimum. The full\n",
        "gradient descent algorithm requires iterating these updates until\n",
        "convergence. The process is:\n",
        "\n",
        "1.  *Initialise* parameters with random or reasonable starting values\n",
        "2.  *Compute gradients* at the current position\n",
        "3.  *Update parameters* using the gradient and learning rate\n",
        "4.  *Repeat* steps 2-3 until convergence (gradients become very small or\n",
        "    error stops decreasing)\n",
        "\n",
        "The beauty of this algorithm is its simplicity and general applicability\n",
        "- the same basic approach works for many different types of models and\n",
        "error functions."
      ],
      "id": "ApklCmPpYPGp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX9ZpQAeYPGp"
      },
      "source": [
        "## Gradient Descent Algorithm\n",
        "\n",
        "Let’s run the complete gradient descent algorithm and visualize how the\n",
        "parameters evolve over multiple iterations. The animation will show the\n",
        "path taken through parameter space as the algorithm navigates toward the\n",
        "minimum of the error surface.\n",
        "\n",
        "Each frame shows:\n",
        "\n",
        "-   *Current position*: The green star indicating our current parameter\n",
        "    estimates\n",
        "-   *Error contours*: The background showing the error landscape\n",
        "-   *Path*: The trajectory we’ve taken from the starting point to the\n",
        "    current position\n",
        "\n",
        "Watch how the algorithm follows a curved path that eventually spiral\n",
        "into the minimum, demonstrating the iterative nature of gradient-based\n",
        "optimization."
      ],
      "id": "DX9ZpQAeYPGp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZkU6utOYPGp"
      },
      "outputs": [],
      "source": [
        "num_plots = plot.regression_contour_fit(x, y, diagrams='./ml')"
      ],
      "id": "YZkU6utOYPGp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlWY_V9fYPGp"
      },
      "outputs": [],
      "source": [
        "import notutils as nu\n",
        "from ipywidgets import IntSlider"
      ],
      "id": "RlWY_V9fYPGp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DxRQvZ8YPGp"
      },
      "outputs": [],
      "source": [
        "nu.display_plots('regression_contour_fit{num:0>3}.svg', directory='./ml', num=IntSlider(0, 0, num_plots, 1))"
      ],
      "id": "-DxRQvZ8YPGp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBBUb7edYPGp"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/regression_contour_fit028.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>Batch gradient descent for linear regression showing the\n",
        "final converged solution.</i>\n",
        "\n",
        "The final frame shows the algorithm has converged to the minimum of the\n",
        "error surface. Notice how:\n",
        "\n",
        "-   *Path shape*: The trajectory follows the natural gradient flow,\n",
        "    starting with larger steps when gradients are large and taking\n",
        "    smaller steps as we approach the minimum\n",
        "-   *Convergence*: The final position should be very close to our true\n",
        "    parameter values ($m_{true} = 1.4$, $c_{true} = -3.1$)\n",
        "-   *Efficiency*: The algorithm finds the optimal solution automatically\n",
        "    without us needing to specify the answer ahead of time\n",
        "\n",
        "But note the limitations of gradient-based optimisation: by following\n",
        "local information (the gradient), we can sometimes approach the minimum\n",
        "only very slowly. To improve convergence we can look to more advanced\n",
        "methods that consider curvature or, in neural networks, methods that use\n",
        "a stochastic approximation to the gradient."
      ],
      "id": "mBBUb7edYPGp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1dWrIh0YPGp"
      },
      "source": [
        "## Stochastic Gradient Descent\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/regression-stochastic-gradient-descent.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/regression-stochastic-gradient-descent.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "While batch gradient descent is possible for small datasets, it becomes\n",
        "computationally prohibitive when dealing with large-scale data. Consider\n",
        "modern internet applications where you might have millions or billions\n",
        "of data points - computing the full gradient by summing over all data\n",
        "points in each iteration would be extremely slow.\n",
        "\n",
        "Stochastic Gradient Descent (SGD) offers an elegant solution by\n",
        "approximating the full gradient using just one (or a few) randomly\n",
        "selected data points at each iteration. This approach:\n",
        "\n",
        "1.  *Scales to large datasets*: Each update requires only O(1)\n",
        "    computation regardless of dataset size\n",
        "2.  *Enables online learning*: We can process data as it arrives,\n",
        "    updating our model incrementally\n",
        "3.  *Provides regularization*: The noise in stochastic updates can help\n",
        "    escape local minima (though less relevant for convex problems like\n",
        "    linear regression)\n",
        "4.  *Mirrors biological learning*: Similar to how the perceptron\n",
        "    processes one example at a time\n",
        "\n",
        "The key insight is that while each individual gradient estimate is\n",
        "noisy, on average it points in the correct direction toward the minimum."
      ],
      "id": "z1dWrIh0YPGp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GI1X-KFYPGq"
      },
      "source": [
        "## Stochastic Gradient Descent\n",
        "\n",
        "To understand how stochastic gradient descent works mathematically,\n",
        "let’s examine how the full batch gradient can be decomposed into\n",
        "individual per-example contributions. This decomposition reveals why we\n",
        "can update parameters using just one data point at a time.\n",
        "\n",
        "The batch gradient is a sum over all data points:\n",
        "$$\\frac{\\text{d}E(m, c)}{\\text{d} m} = -2\\sum_{i=1}^nx_i(y_i - mx_i - c)$$\n",
        "\n",
        "When we substitute this into our parameter update equation, we get:\n",
        "$$m_\\text{new} \\leftarrow m_\\text{old} + 2\\eta\\left[\\sum_{i=1}^nx_i (y_i - m_\\text{old}x_i - c_\\text{old})\\right]$$\n",
        "\n",
        "The key insight is that this single large update is mathematically\n",
        "equivalent to a sequence of smaller updates, each using just one data\n",
        "point. However, there’s an important difference in practice - in SGD, we\n",
        "update the parameters after each individual contribution rather than\n",
        "keeping them fixed throughout the sum."
      ],
      "id": "7GI1X-KFYPGq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3427xwtZYPGq"
      },
      "source": [
        "## Updating $c$ and $m$\n",
        "\n",
        "Here’s where stochastic gradient descent fundamentally differs from\n",
        "batch gradient descent. In batch gradient descent, we compute the full\n",
        "gradient using fixed parameter values $m_{\\text{old}}$ and\n",
        "$c_{\\text{old}}$ for all data points, then make one large update.\n",
        "\n",
        "In stochastic gradient descent, we update the parameters after\n",
        "processing each individual data point. This means:\n",
        "\n",
        "1.  *Dynamic parameters*: The values of $m$ and $c$ change as we process\n",
        "    each example, so later examples in the sequence see updated\n",
        "    parameter values\n",
        "2.  *Approximate gradient*: Since parameters change during the “sum,”\n",
        "    we’re not computing the true batch gradient - we’re using a\n",
        "    stochastic approximation\n",
        "3.  *Online processing*: We can process data points as they arrive,\n",
        "    making the algorithm suitable for streaming data\n",
        "\n",
        "This dynamic updating is what makes SGD both powerful and noisy. The\n",
        "noise can actually be beneficial - it provides a form of regularization\n",
        "and can help escape poor local minima in more complex problems."
      ],
      "id": "3427xwtZYPGq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "411VAxK7YPGq"
      },
      "source": [
        "## Stochastic Gradient Descent\n",
        "\n",
        "In practice, stochastic gradient descent simplifies to a very clean\n",
        "algorithm. Since we process data points in random order, each SGD update\n",
        "uses the gradient contribution from just one randomly selected example.\n",
        "The update equations become:\n",
        "\n",
        "For the slope parameter $$\n",
        "m_\\text{new} = m_\\text{old} + 2\\eta\\left[x_i (y_i - m_\\text{old}x_i - c_\\text{old})\\right].\n",
        "$$ For the intercept parameter\n",
        "$$c_\\text{new} = c_\\text{old} + 2\\eta\\left[(y_i - m_\\text{old}x_i - c_\\text{old})\\right].\n",
        "$$ Notice how these look exactly like the per-example terms from our\n",
        "batch gradient, but now we apply them immediately rather than\n",
        "accumulating them in a sum.\n",
        "\n",
        "The implementation is straightforward: randomly select a data point,\n",
        "compute its contribution to the gradient, and immediately update the\n",
        "parameters. This process repeats for many iterations, with each\n",
        "iteration using a different random example."
      ],
      "id": "411VAxK7YPGq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUB8_E1YYPGq"
      },
      "outputs": [],
      "source": [
        "# choose a random point for the update\n",
        "i = np.random.randint(x.shape[0]-1)\n",
        "# update m\n",
        "m_star = m_star + 2*learn_rate*(x[i]*(y[i]-m_star*x[i] - c_star))\n",
        "# update c\n",
        "c_star = c_star + 2*learn_rate*(y[i]-m_star*x[i] - c_star)"
      ],
      "id": "CUB8_E1YYPGq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FNYd03RYPGq"
      },
      "source": [
        "## SGD for Linear Regression\n",
        "\n",
        "Let’s now run the complete stochastic gradient descent algorithm and\n",
        "visualize how it differs from batch gradient descent. The key\n",
        "differences you’ll observe:\n",
        "\n",
        "1.  *Noisier path*: SGD takes a much more erratic path through parameter\n",
        "    space because each update is based on just one noisy example\n",
        "2.  *More iterations needed*: While each iteration is faster ($O(1)$ vs\n",
        "    $O(n)$), SGD typically needs more iterations to converge\n",
        "3.  *Never fully converges*: SGD will continue to “bounce around” near\n",
        "    the minimum due to the noise in gradient estimates\n",
        "4.  *Practical efficiency*: Despite the noise, SGD often reaches good\n",
        "    solutions faster in wall-clock time for large datasets\n",
        "\n",
        "The animation shows how the stochastic approximation creates a much more\n",
        "chaotic but ultimately effective optimization trajectory."
      ],
      "id": "5FNYd03RYPGq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWKCo2BBYPGq"
      },
      "outputs": [],
      "source": [
        "num_plots = plot.regression_contour_sgd(x, y, diagrams='./ml')"
      ],
      "id": "QWKCo2BBYPGq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAYyagEdYPGr"
      },
      "outputs": [],
      "source": [
        "import notutils as nu\n",
        "from ipywidgets import IntSlider"
      ],
      "id": "zAYyagEdYPGr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EF_cPEJIYPGr"
      },
      "outputs": [],
      "source": [
        "import notutils as nu"
      ],
      "id": "EF_cPEJIYPGr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YVYq1E1YPGr"
      },
      "outputs": [],
      "source": [
        "nu.display_plots('regression_sgd_contour_fit{num:0>3}.svg',\n",
        "    directory='./ml', num=IntSlider(0, 0, num_plots, 1))"
      ],
      "id": "3YVYq1E1YPGr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiXs-LM9YPGr"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//ml/regression_sgd_contour_fit058.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>Stochastic gradient descent for linear regression showing the\n",
        "noisy but effective optimization path.</i>\n",
        "\n",
        "The final visualisation demonstrates the behaviour of SGD compared to\n",
        "batch gradient descent:\n",
        "\n",
        "**Optimization Path Characteristics:**\n",
        "\n",
        "-   *Erratic trajectory*: The path zigzags chaotically through parameter\n",
        "    space rather than following smooth contours\n",
        "-   *Local exploration*: The algorithm continues to explore around the\n",
        "    minimum even after finding a good solution\n",
        "-   *Effective convergence*: Despite the noise, SGD successfully\n",
        "    navigates to the optimal region\n",
        "\n",
        "**Practical Implications:**\n",
        "\n",
        "-   *Computational efficiency*: Each SGD iteration processes just one\n",
        "    example, making it ideal for large datasets where computing the full\n",
        "    gradient is prohibitive\n",
        "-   *Online learning capability*: The algorithm can adapt to new data\n",
        "    points as they arrive, enabling real-time model updates\n",
        "-   *Regularization effect*: The noise in SGD can prevent overfitting\n",
        "    and help escape poor local minima in more complex optimization\n",
        "    landscapes\n",
        "\n",
        "**Trade-offs:**\n",
        "\n",
        "-   *Convergence vs. efficiency*: SGD trades smooth convergence for\n",
        "    computational speed - it may never fully settle but reaches good\n",
        "    solutions quickly\n",
        "-   *Hyperparameter sensitivity*: Learning rate tuning becomes more\n",
        "    critical as too large values can cause divergence, while too small\n",
        "    values slow convergence excessively\n",
        "\n",
        "This stochastic approach forms the foundation for training modern\n",
        "machine learning models, particularly deep neural networks where batch\n",
        "gradient computation is computationally intractable."
      ],
      "id": "qiXs-LM9YPGr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qmn-bexfYPGr"
      },
      "source": [
        "## Mini-Batch\n",
        "\n",
        "In practice we normally use mini-batches for gradient descent. Rather\n",
        "than computing the gradient with respect to one point, you compute\n",
        "gradient with respect to a small batch of points."
      ],
      "id": "Qmn-bexfYPGr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75-CXnvdYPGr"
      },
      "source": [
        "### Exercise 1\n",
        "\n",
        "Can you write your own implementation of mini-batch gradient ascent for\n",
        "the regression problem? Allow the user to choose the mini-batch size."
      ],
      "id": "75-CXnvdYPGr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSJPSjcUYPGs"
      },
      "outputs": [],
      "source": [
        "# Write your answer to Exercise 1 here\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "YSJPSjcUYPGs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2qvHiA1YPGs"
      },
      "source": [
        "## Reflection on Linear Regression and Supervised Learning\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-regression-iterative.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/linear-regression-iterative.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "Think about:\n",
        "\n",
        "1.  What effect does the learning rate have in the optimization?\n",
        "\n",
        "-   What’s the effect of making it too small?\n",
        "-   What’s the effect of making it too big?\n",
        "\n",
        "1.  Do you get the same result for both stochastic and steepest gradient\n",
        "    descent?\n",
        "\n",
        "2.  Are there any advantages for stochastic gradient descent for small\n",
        "    data?\n",
        "\n",
        "for loss functions."
      ],
      "id": "e2qvHiA1YPGs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWiC2Xu7YPGs"
      },
      "source": [
        "## Thanks!\n",
        "\n",
        "For more information on these subjects and more you might want to check\n",
        "the following resources.\n",
        "\n",
        "-   book: [The Atomic\n",
        "    Human](https://www.penguin.co.uk/books/455130/the-atomic-human-by-lawrence-neil-d/9780241625248)\n",
        "-   twitter: [@lawrennd](https://twitter.com/lawrennd)\n",
        "-   podcast: [The Talking Machines](http://thetalkingmachines.com)\n",
        "-   newspaper: [Guardian Profile\n",
        "    Page](http://www.theguardian.com/profile/neil-lawrence)\n",
        "-   blog:\n",
        "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)"
      ],
      "id": "nWiC2Xu7YPGs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAD9J5fRYPGs"
      },
      "source": [
        "## Further Reading\n",
        "\n",
        "-   Section 1.1.3 of Rogers and Girolami (2011)"
      ],
      "id": "jAD9J5fRYPGs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM1M94PrYPGs"
      },
      "source": [
        "::: {.cell .markdown}\n",
        "\n",
        "## References\n",
        "\n",
        "Rogers, S., Girolami, M., 2011. A first course in machine learning. CRC\n",
        "Press.\n",
        "\n",
        "Rosenblatt, F., 1958. The perceptron: A probabilistic model for\n",
        "information storage and organization in the brain. Psychological Review\n",
        "65, 386–408. <https://doi.org/10.1037/h0042519>"
      ],
      "id": "dM1M94PrYPGs"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  }
}